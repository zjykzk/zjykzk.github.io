<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rocketmq on 老K随笔</title>
    <link>http://zjykzk.github.io/tags/rocketmq/</link>
    <description>Recent content in Rocketmq on 老K随笔</description>
    <generator>Hugo</generator>
    <language>zh</language>
    <copyright>(c) 2017 zenk.</copyright>
    <lastBuildDate>Fri, 25 Jan 2019 15:35:52 +0800</lastBuildDate>
    <atom:link href="http://zjykzk.github.io/tags/rocketmq/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RocketMQ HA实现</title>
      <link>http://zjykzk.github.io/posts/cs/rocketmq/ha/</link>
      <pubDate>Fri, 25 Jan 2019 15:35:52 +0800</pubDate>
      <guid>http://zjykzk.github.io/posts/cs/rocketmq/ha/</guid>
      <description>&lt;h2 id=&#34;ha原理&#34;&gt;HA原理&lt;/h2&gt;&#xA;&lt;p&gt;RocketMQ支持主结点的数据同步到从结点。同步的数据依赖于当前从结点的状态。从结点连接到主结点的时候会上报自己的当前commitlog的最大偏移量。主结点收到以后会根据这个值计算出传输的起始位置，如果上报的commitlog的最大偏移量：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;等于0，主结点会从当前最大偏移量减去一个log文件大小那个位置开始传输。如果小于0，那么从0开始传输。&lt;/li&gt;&#xA;&lt;li&gt;大于0，从该值开始传输。&lt;/li&gt;&#xA;&lt;li&gt;小于0，这种情况不存在。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;所以，这里我们可以知道如果从结点已经就有数据情况，如果数据不是从主结点同步过来的，那么同步之后就会有问题了。比如说：从结点已经有10000条数据，同时某个topic，暂时就叫&lt;strong&gt;OLD_TOPIC&lt;/strong&gt;的&lt;em&gt;消费队列0&lt;/em&gt;长度1000。这个时候，主结点就会从第10000条数据开始同步，可能会发送几种情况：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;主结点没有10000数据，那么就不会同步数据，造成从结点上面数据丢失。&lt;/li&gt;&#xA;&lt;li&gt;主结点有超过10000数据，但是它的&lt;strong&gt;OLD_TOPIC&lt;/strong&gt;的&lt;em&gt;消费队列0&lt;/em&gt;的长度小于1000，那么同步过来的数据就会覆盖原来的数据。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;所以，从结点的初始状态需要从0开始或者本来就是和主同步过的状态。因此，在删除topic的时候从结点要保证删除干净，不然从结点就会脏数据，影响消费。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;为什么这样同步不会有问题呢？&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;那是因为同步的数据里面包含了具体消费队列ID，队列中的偏移量以及消息的偏移量，所以同步的时候能够写到同一个位置。&lt;/p&gt;&#xA;&lt;h2 id=&#34;主结点同步逻辑&#34;&gt;主结点同步逻辑&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/ha-master.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;发送一条消息的时候，在开启&lt;strong&gt;SYNC_MASTER&lt;/strong&gt;情况下，需要四个线程合作才能完成消息的发送。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;SendMessageProcessor&lt;/strong&gt;负责处理接收发送消息的请求并落盘（异步或者同步），接着向&lt;strong&gt;GroupTransferService&lt;/strong&gt;发送等待同步完成的请求，然后等待知道超时或者&lt;strong&gt;GroupTransferService&lt;/strong&gt;通知同步完成。同时，还会同时&lt;strong&gt;WriteSocketService&lt;/strong&gt;有数据可以写了。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;WriteSocketService&lt;/strong&gt;负责根据从结点上报的位置（变量&lt;code&gt;slaveRequestOffset&lt;/code&gt;），不断的向从结点传输数据。同时会维护和从结点的一个心跳，如果一段时间没有通不过数据，就会发送一个消息头，包含当前同步的起始位置。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;GroupTransferService&lt;/strong&gt;不断的轮询比较当前已经被从结点同步的最大偏移（变量&lt;code&gt;push2SlaveMaxOffset&lt;/code&gt;）和&lt;strong&gt;SendMessageProcessor&lt;/strong&gt;发送过来的请求中包含的偏移量，如果大于或者等于就会通知&lt;strong&gt;SendMessageProcessor&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ReadSocketService&lt;/strong&gt;负责读取从结点上报上来的同步偏移量。更新变量&lt;code&gt;push2SlaveMaxOffset&lt;/code&gt;和&lt;code&gt;slaveRequestOffset&lt;/code&gt;并通知&lt;strong&gt;GroupTransferService&lt;/strong&gt;。从而，它也会影响&lt;strong&gt;WriteSocketService&lt;/strong&gt;的行为。同时，它还维护着和从结点连接的过期工作，如果超过指定时间没有收到消息就会断开连接，同时会停止&lt;strong&gt;WriteSocketService&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;从结点同步逻辑&#34;&gt;从结点同步逻辑&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/ha-slave.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;从结点的同步逻辑相对简单主要做几件事情：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;管理和主结点的连接，如果超过一段时间没有收到主点结点的数据，就会断开连接。这个时间戳保存在变量&lt;code&gt;lastWriteTimestamp&lt;/code&gt;中，刚刚连接上主结点和从主结点读到数据都会更新该变量。&lt;/li&gt;&#xA;&lt;li&gt;上报当前commitlog的最大偏移量，该行为会发生三个地方：a.写完一个消息；b.处理完当前收到的所有数据；c.一段时间内没有收到主结点的数据。&lt;/li&gt;&#xA;&lt;li&gt;维护收到的数据。这里有两个接收数据的buffer，主要方便处理当一个buffer的空间用完以后处理剩余的消息。一个buffer的情况下，先拷贝到一个临时byte数据，然后再拷贝回去，需要两次内存拷贝。如果两个buffer只需要一次拷贝。&lt;/li&gt;&#xA;&lt;li&gt;写消息。把从主结点同步过来的数据写到磁盘。收到数据的时候会判断主结点发过来的偏移量是否等于自己当前的偏移量如果不一样就会断开和主结点的连接。&lt;/li&gt;&#xA;&lt;li&gt;任何从连接中读数据的时候如果有错误就会断开连接。&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>RocketMQ push模式的实现细节</title>
      <link>http://zjykzk.github.io/posts/cs/rocketmq/push-consumer/</link>
      <pubDate>Wed, 16 Jan 2019 16:54:09 +0800</pubDate>
      <guid>http://zjykzk.github.io/posts/cs/rocketmq/push-consumer/</guid>
      <description>&lt;p&gt;Rocketmq使用常轮询的方式实现了push功能。主要包括几个组件：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DefaultMQPushConsumerImpl：拉消息的类型。&lt;/li&gt;&#xA;&lt;li&gt;ProcessQueue：保存拉出来的消息。&lt;/li&gt;&#xA;&lt;li&gt;PullMessageService：执行拉消息服务。&lt;/li&gt;&#xA;&lt;li&gt;ConsumeMessageService：消费消息服务。&lt;/li&gt;&#xA;&lt;li&gt;ReblanceService：负载均衡服务。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;类关系&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/push-consumer-class.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;（真想吐槽！）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;执行过程&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/push-consumer-active.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;defaultmqpushconsumerimpl&#34;&gt;DefaultMQPushConsumerImpl&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;DefaultMQPushConsumerImpl&lt;/code&gt;实现了消费者的接口。同时是个启动者，通过它直接或间接启动了拉消息服务，消费消息服务。&lt;/p&gt;&#xA;&lt;p&gt;其中提供了一个重要的接口&lt;code&gt;pullMessage&lt;/code&gt;。该接口的流程如下：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/push-consumer-pull.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;在拉消息过程中，做了流控，防止拉的太快，消费的太慢。主要从三个方面检测：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;从某个消费队列拉取的等待消费的消息数量。如果超过阀值，延迟50ms后再次拉取消息。阀值默认是1000。如果设置了topic级别的阀值（默认没有限制），在队列负载均衡以后会重新计算，具体为topic级别的阀值除以当前负责的消费队列数量。主要配置变量：&lt;code&gt;DefaultMQPushConsumerImpl.pullThresholdForQueue&lt;/code&gt;和&lt;code&gt;DefaultMQPushConsumerImpl.pullThresholdForTopic&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;li&gt;从某个消费队列拉取的等待消费的消息大小（只考虑body）。同样，超过阀值就会延迟50ms后再次拉取消息。阀值默认是100M。如果topic设置了级别（默认没有限制），队列负载均衡以后会重新计算队列的限制，具体为topic级别的阀值除以当前负责的消费队列数量。主要配置变量：&lt;code&gt;DefaultMQPushConsumerImpl.pullThresholdSizeForQueue&lt;/code&gt;和&lt;code&gt;DefaultMQPushConsumerImpl.pullThresholdSizeForTopic&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;li&gt;在并发消费模式下，从某个消费队列拉取的等待消费的消息中，在消费队列中的最大位置和最小位置之间差别。如果超过阀值，也会延迟50ms后再拉取消息。默认是2000，这里可能会存在误判。因为，有条件拉取消息的时候，是有可能出现同一个消费队列中拉到的两个消息在队列中的位置距离很远。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;几个考虑：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;NO_NEW_MSG/NO_MATCHED_MSG&lt;/code&gt;情况下，&lt;code&gt;correctTagsOffset&lt;/code&gt;的逻辑为什么需要考虑有没有消息？如果还有消息说明本地还没有消息没被消费，此时更新的offset是服务端返回的，存在比没有被消费的消息偏移量大的情况。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;OFFSET_ILLEAGL&lt;/code&gt;的情况下为什么要过10s以后才去更新offsetstore，保存offset，在reblance中移除process queue？出现这个问题是因为&lt;code&gt;NO_MATCHED_LOGIC_QUEUE/NO_MESSAGE_IN_QUEUE/OFFSET_OVERFLOW_BADLY/OFFSET_TOO_SMALL&lt;/code&gt;这四种情况，而这些情况可能发生在服务端在恢复数据的时候，因此考虑是暂停消费这个队列。如果drop之后不延迟，就会有可能又去拉取消息了。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;processqueue&#34;&gt;ProcessQueue&lt;/h2&gt;&#xA;&lt;p&gt;保存push的消费者拉到的消息。同时，有序消费模式还记录了情况下正在消费的消息。&lt;/p&gt;&#xA;&lt;h2 id=&#34;pullmessageservice&#34;&gt;PullMessageService&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;PullMessageService&lt;/code&gt;只负责拉取消息，它会调用&lt;code&gt;DefaultMQPushConsumerImpl.pullMessage&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;p&gt;当&lt;code&gt;ReblanceService&lt;/code&gt;执行负载均衡的时候如果发现被分配了新的消息队列就会最终调用&lt;code&gt;PullMessage.executePullRequestImmediately&lt;/code&gt;执行拉取消息。代码执行路径：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ReblanceService.run&#xA;-&amp;gt;MQClientInstance.doReblance&#xA;-&amp;gt;MQConsumerInnter.doReblance[DefaultMQPushConsumerImpl.doReblance]&#xA;-&amp;gt;ReblanceImpl.doReblance&#xA;-&amp;gt;ReblanceImpl.dispatchPullRequest[ReblancePushImpl.dispatchPullRequest]&#xA;-&amp;gt;DefaultMQPushConsumerImpl.executePullRequestImmediately&#xA;-&amp;gt;PullMessage.executePullRequestImmediately&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外，在&lt;code&gt;DefaultMQPushConsumerImpl.pullMessage&lt;/code&gt;执行时，也会根据条件调用&lt;code&gt;PullMessageService.executePullRequestImmediately&lt;/code&gt;、&lt;code&gt;PullMessageService.executeTaskLater&lt;/code&gt;或者&lt;code&gt;PullMessageService.executePullRequestLater&lt;/code&gt;触发拉取消息。&lt;/p&gt;&#xA;&lt;h2 id=&#34;consumemessageservice&#34;&gt;ConsumeMessageService&lt;/h2&gt;&#xA;&lt;p&gt;消费服务分并发消费和顺序消费，主要区别在于提交消费任务逻辑，消费逻辑和处理消费结果的逻辑，以及对message queue的处理逻辑。另外，顺序消费是指在同一个消费队列里面的消息顺序消费。&lt;/p&gt;&#xA;&lt;h3 id=&#34;提交消费任务&#34;&gt;提交消费任务&lt;/h3&gt;&#xA;&lt;p&gt;并发消费：把消息分成多个批次并发处理，一批多少个消息是自定义的，默认是1。如果提交异常，则延迟5s后提交。&lt;/p&gt;&#xA;&lt;p&gt;顺序消费：依赖于process queue是否正在被消费，这样避免同时消费多个不同的消息，不然就没法保证有序了。&lt;/p&gt;&#xA;&lt;h3 id=&#34;消费逻辑&#34;&gt;消费逻辑&lt;/h3&gt;&#xA;&lt;p&gt;下图中左边是&lt;em&gt;并发消费&lt;/em&gt;，右边是&lt;em&gt;顺序消费&lt;/em&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/push-consumer-consume.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;消费消息的时候，在可能停顿的执行点上面都加上了process queue是否已经drop的检查。&lt;/p&gt;&#xA;&lt;p&gt;因为提交任务的方式不一样导致了不同模式下面消费逻辑的差别。&lt;/p&gt;&#xA;&lt;p&gt;并发消费：只考虑当前的消息即可。&lt;/p&gt;&#xA;&lt;p&gt;顺序消费：从process queue中取消息。消费的时候需要确保：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;每个消费队列某一时候只有一个消费请求被执行。&lt;/li&gt;&#xA;&lt;li&gt;每个消费队列某一时刻只有一个地方在执行用户的消费逻辑。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;以上两个条件中只要一个条件不满足，就没法保证消息顺序消费。另外，第一个逻辑需要的锁，是因为消费慢，同时队列被分配别的消费者，在消费结束之前又分配回来了，就有可能导致1条件不满足，所以需要加锁。在代码层面第一个逻辑需要的锁已经确保了第二个逻辑。消费之前需要锁的原因是为了避免，用户还在消费的时候向broker解锁。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;锁的逻辑&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;只有message queue被锁住了才能消费。客户端向服务端发送锁的请求成功以后才算锁成功。同时锁会有一个过期时间。在客户端这边定时向broker发送锁的请求，所得粒度是group+clientID，过期时间是30s。在服务端这边，锁了的过期时间是60s，这个时间以后能够接收其他锁的请求。&lt;/p&gt;&#xA;&lt;p&gt;在负载均衡的时候，检查一个消费队列发现不属于自己或者长时间没有拉的时候就会把这个消费队列移除掉。移除的逻辑比较有意思，为了确保这个消费队列正在被消费不会被移除，这里使用了一个消费锁。移除的时候尝试获得这个锁，如果超过1s还没有获得就会等待下一次负载均衡的检查，如果获得了锁就会延迟20s再向broker发送解锁请求。这里的延迟，有个效果就是可能这时候已经向broker发送了拉消息的请求，如果在它返回之前又把队列分配给自己了，那么就有可能两个触发一个拉消息的请求，这个时候就会同时有两个拉消息的请求，那么拉出来重复的消息。&lt;/p&gt;&#xA;&lt;h3 id=&#34;处理消费结果&#34;&gt;处理消费结果&lt;/h3&gt;&#xA;&lt;p&gt;下图中左边是并发消费，右边是顺序消费。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://zjykzk.github.io/imgs/rocketmq/push-consumer-result.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;处理消费结果的逻辑主要是处理消费失败的消息。&lt;/p&gt;&#xA;&lt;p&gt;并发消费：如果是在广播模式下，直接丢弃了。如果是在集群模式下面会尝试把消息发回broker，如果发送失败的话，就会把这些发送失败的消息延迟提交消费。&lt;/p&gt;&#xA;&lt;p&gt;顺序模式：如果是&lt;code&gt;ROLLBACK&lt;/code&gt;，把消息放回，再次消费。如果是&lt;code&gt;SUSPEND_CURRENT_QUEUE_A_MOMENT&lt;/code&gt;则会判断是否需要停止一段时间再消费。通过检查消费次数，当超过预定的值（默认是没有限制）就会把消息发回broker。如果消息都已经发回broker，就提交消息接下去消费，否则就停一会，把当前的消息延迟提交消费。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;处理message queue&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;并发消费：定时清理长时间没法消费的消息，默认是15分钟。&lt;/p&gt;&#xA;&lt;p&gt;顺序消费：在集群模式下面，定时向broker锁住message queue，锁的粒度是group+clientID。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RocketMQ offset管理</title>
      <link>http://zjykzk.github.io/posts/cs/rocketmq/offset/</link>
      <pubDate>Fri, 28 Dec 2018 16:03:51 +0800</pubDate>
      <guid>http://zjykzk.github.io/posts/cs/rocketmq/offset/</guid>
      <description>&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;&#xA;&lt;p&gt;记录每个消费队列的消费进度。以topic，group为单位。&lt;/p&gt;&#xA;&lt;h2 id=&#34;类型&#34;&gt;类型&lt;/h2&gt;&#xA;&lt;p&gt;根据保存的位置可以分为本地和远程两种类型。本地类型就是以文本文件的形式保存在客户端，内容是非正式的json数据，而远程类型是指数据保存在broker服务器上面，内容同样是非正式的json数据。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;代码&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;本地类型：&lt;code&gt;org.apache.rocketmq.client.consumer.store.LocalFileOffsetStore&lt;/code&gt;。&#xA;远程类型：&lt;code&gt;org.apache.rocketmq.client.consumer.store.RemoteBrokerOffsetStore&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;使用&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;默认情况，当消费模式是&lt;em&gt;广播&lt;/em&gt;的时候使用&lt;em&gt;本地类型&lt;/em&gt;，因为每个消费者管理自己的进度，而且是所有消费队列的进度，各个消费者之间也不会有消费进度的交集。当消费模式是&lt;em&gt;集群&lt;/em&gt;的时候使用&lt;em&gt;远程类型&lt;/em&gt;，因为消息被多个消费者消费，每个消费者只负责消费其中部分消费队列，在添加、删除消费者的时候，原来消费者负责的消费队列会动态变化，因此需要集中管理消费进度，不然就冲突了。&lt;/p&gt;&#xA;&lt;p&gt;但是，代码中依然提供了接口，让用户自己指定类型，比如可以保存数据到monogodb。&lt;/p&gt;&#xA;&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;本地类型&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;数据保存在&lt;code&gt;$storeDir/.rocketmq_offsets/$clientID/$group/offsets.json&lt;/code&gt;中，里面的数据是非标准json数据，用的是阿里的fastjson这个库。其中&lt;code&gt;$storeDir&lt;/code&gt;是可以通过系统变量&lt;code&gt;rocketmq.client.localOffsetStoreDir&lt;/code&gt;配置，如果没有指定参数就使用HOME目录。&lt;code&gt;$clientID&lt;/code&gt;和&lt;code&gt;$group&lt;/code&gt;分别表示消费者的id和分组。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// example&#xA;{&amp;#34;offsetTable&amp;#34;:{{&amp;#34;brokerName&amp;#34;:&amp;#34;topic&amp;#34;,&amp;#34;queueId&amp;#34;:1,&amp;#34;topic&amp;#34;:&amp;#34;broker&amp;#34;}:0}}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;远程类型&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;数据保存在&lt;code&gt;$rootPath/config/consumerOffset.json&lt;/code&gt;文件中，里面的数据是非标准json数据，用的是阿里的fastjson这个库。&lt;code&gt;offsetTable&lt;/code&gt;中的key格式是&lt;code&gt;topic@group&lt;/code&gt;，value格式&lt;code&gt;queueID:offset&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// example&#xA;{&#xA;    &amp;#34;offsetTable&amp;#34;:{&#xA;        &amp;#34;test@benchmark_consumer_61&amp;#34;:{&#xA;            0:5280,1:5312,2:5312,3:5312&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;接口&#34;&gt;接口&lt;/h2&gt;&#xA;&lt;p&gt;通过接口类型&lt;code&gt;org.apache.rocketmq.client.consumer.store.OffsetStore&lt;/code&gt;抽象了消费进度的相关操作。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;load&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;在消费者启动的时候，需要把消费进度载入内存。只有本地类型会载入数据。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;updateOffset&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;更新消费队列的进度。可以选择在比当前消费进度大的时候才更新，这个目的主要用于push模式下面消息是并发消费的，这样每批消息完成以后更新进度是并发，可能会导致进度低的晚于进度高的更新，这个模式就是为了避免这个情况。代码在类&lt;code&gt;ConsumeMessageConcurrentlyService&lt;/code&gt;中。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;readOffset&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;读取消费队列的消费进度，数据存在内存和存储（本地或者broker服务）中，提供了三种读取的方式：1.内存；2.存储；3.先内存，如果没有后存储。在两个地方的实现中，从存储中读到数据以后会更新到内存。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;persistAll&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;持久化指定的多个消费队列的消费进度。本地类型的实现中只会持久化内存中的消费进度。远程类型除此之外，还会把指定的消费队列以外的那些队列从内存中移除。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;persist&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;持久化指定的单个消费队列的消费进度。只有远程类型实现了该接口。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;removeOffset&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;移除某个消费队列的消费进度。只有远程类型实现了该接口。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;updateConsumeOffsetToBroker&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;更新消费队列到broker服务，只有远程类型实现了该接口。（这个设计好尴尬，本地类型需要么。。。）&lt;/p&gt;&#xA;&lt;h2 id=&#34;管理&#34;&gt;管理&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;org.apache.rocketmq.client.impl.consumer.RebalanceImpl.updateProcessQueueTableInRebalance&lt;/code&gt;做消费的负载均衡时，会对消费进度做管理。这个过程通过对比新分配的消费队列（简称新队列）和&lt;code&gt;org.apache.rocketmq.client.impl.consumer.RebalanceImpl.processQueueTable&lt;/code&gt;维护的消费队列（简称旧队列），有几种情况：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;如果旧队列的消费队列不在新队列中，那么就会先持久化该队列的消费进度，再做删除操作。&lt;em&gt;push模式同时优势有序的集群消费还需要做外的事情。&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;如果如果旧队列的消费队列在新队列中，&lt;em&gt;push模式下检查是否过期，过期的化先持久化，再删除进度。&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;如果新队列的消费队列不在旧队列中，删除消费进度。本地模式不会做删除操作，远程模式会把内存中的消费进度删除掉。同时，push模式下面会从存储中拉取消费进度并保存到内存。&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>slave和master同步连接经常重连，导致发送消息失败</title>
      <link>http://zjykzk.github.io/posts/cs/rocketmq/slave-sync-from-master-disconnect/</link>
      <pubDate>Mon, 22 Oct 2018 17:07:02 +0800</pubDate>
      <guid>http://zjykzk.github.io/posts/cs/rocketmq/slave-sync-from-master-disconnect/</guid>
      <description>&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;&#xA;&lt;p&gt;封装RocketMQ的组件boots-broker每天都返回几个的500。排查发现是因为slave向master同步消息的时候，由于没有及时向master报告自己的同步进度，从而master没有向slave及时同步消息，导致消息发送失败。&lt;/p&gt;&#xA;&lt;h2 id=&#34;排查过程&#34;&gt;排查过程&lt;/h2&gt;&#xA;&lt;p&gt;查看boots-broker日志，发现问题日志：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[TIMEOUT_CLEAN_QUEUE]broker busy, start flow control for a while, period in queue: 1008ms, size of queue: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明，RocketMQ处理发送消息比较慢。可是，从&lt;code&gt;size of queue&lt;/code&gt;可以看出，堆积的消息为0。&lt;/p&gt;&#xA;&lt;p&gt;查看机器资源消耗情况，发现资源都是充裕的。&lt;/p&gt;&#xA;&lt;p&gt;查看RocketMQ日志，发现store.log中有异常，master中的store.log周期性的发生以下日志：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2018-10-22 15:44:07 INFO AcceptSocketService - HAService receive new connection, /10.38.34.27:54052&#xA;2018-10-22 15:44:07 INFO ReadSocketService - ReadSocketService service started&#xA;2018-10-22 15:44:07 INFO WriteSocketService - WriteSocketService service started&#xA;2018-10-22 15:44:08 INFO WriteSocketService - WriteSocketService service end&#xA;2018-10-22 15:44:12 INFO ReadSocketService - slave[/10.38.34.27:54052] request offset 157843228&#xA;2018-10-22 15:44:12 INFO WriteSocketService - master transfer data from 157843228 to slave[/10.38.34.27:54052], and slave request 157843228&#xA;2018-10-22 15:44:33 WARN ReadSocketService - ha housekeeping, found this connection[/10.38.34.27:54052] expired, 20019&#xA;2018-10-22 15:44:33 INFO ReadSocketService - ReadSocketService service end&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上可以看出，slave主动向master建立连接，5s之后发送自己当前同步的进度，master收到以后向slave发送同步数据，最后master由于slave的连接过期，主动断开连接。&lt;/p&gt;</description>
    </item>
    <item>
      <title>rocketmq store模块</title>
      <link>http://zjykzk.github.io/posts/cs/rocketmq/store/</link>
      <pubDate>Fri, 08 Dec 2017 17:59:56 +0800</pubDate>
      <guid>http://zjykzk.github.io/posts/cs/rocketmq/store/</guid>
      <description>&lt;h2 id=&#34;功能&#34;&gt;功能&lt;/h2&gt;&#xA;&lt;p&gt;store模块是rocketmq的核心模块。主要功能有：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;消息存储&lt;/li&gt;&#xA;&lt;li&gt;消息索引&lt;/li&gt;&#xA;&lt;li&gt;消费队列&lt;/li&gt;&#xA;&lt;li&gt;主从同步&lt;/li&gt;&#xA;&lt;li&gt;延迟消息&lt;/li&gt;&#xA;&lt;li&gt;清理过期的消息和消费队列&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;消息存储&#34;&gt;消息存储&lt;/h2&gt;&#xA;&lt;p&gt;负责消息存储，包括写消息，刷盘。&lt;/p&gt;&#xA;&lt;h3 id=&#34;消息文件&#34;&gt;消息文件&lt;/h3&gt;&#xA;&lt;p&gt;消息保存在默认值为&lt;code&gt;${user.home}\store\commitlog&lt;/code&gt;文件夹下，可以通过配置项&lt;code&gt;storePathCommitLog&lt;/code&gt;修改。所有的消息都写入一个逻辑文件，每个逻辑文件包含大小相等的物理文件。&lt;/p&gt;&#xA;&lt;h3 id=&#34;写消息&#34;&gt;写消息&lt;/h3&gt;&#xA;&lt;p&gt;写消息在不同的场景下面会有不同的逻辑。&lt;/p&gt;&#xA;&lt;h4 id=&#34;同步刷盘&#34;&gt;同步刷盘&lt;/h4&gt;&#xA;&lt;p&gt;每条消息要写到磁盘以后才算完成。&lt;/p&gt;&#xA;&lt;p&gt;在同步刷盘的场景下，会有一个定期检查消息是否已经写入磁盘的线程：&lt;code&gt;GroupCommitService&lt;/code&gt;，除了检查还会进行刷盘的操作 。写消息的时候会生成一个&lt;code&gt;GroupCommitRequest&lt;/code&gt;提交到&lt;code&gt;GroupCommitService&lt;/code&gt;，并等待被唤醒或者超时。当&lt;code&gt;GroupCommitService&lt;/code&gt;发现已经刷盘的最后一个消息的索引大于等于本消息的索引时就会唤醒&lt;code&gt;GroupCommitRequest&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;：以上的场景还依赖于消息的属性&lt;code&gt;WAIT&lt;/code&gt;，只有该属性为空或者为&lt;code&gt;true&lt;/code&gt;才会执行同步刷盘逻辑，默认是空的。&lt;/p&gt;&#xA;&lt;h4 id=&#34;异步刷盘&#34;&gt;异步刷盘&lt;/h4&gt;&#xA;&lt;p&gt;在异步刷盘的场景下，会有一个把数据刷到磁盘的辅助线程：&lt;code&gt;FlushRealTimeService&lt;/code&gt;。写消息仅仅唤醒该线程就结束了写盘操作。&lt;/p&gt;&#xA;&lt;h4 id=&#34;主从同步&#34;&gt;主从同步&lt;/h4&gt;&#xA;&lt;p&gt;每条消息要等一个从broker同步完才算完成。&lt;/p&gt;&#xA;&lt;p&gt;在主从同步的场景下，会有一个定期检查消息是否已经被从broker同步的辅助线程：&lt;code&gt;GroupTransferService&lt;/code&gt;。写消息的时候会生成一个&lt;code&gt;GroupCommitRequest&lt;/code&gt;提交给&lt;code&gt;GroupTransferService&lt;/code&gt;，并等待被唤醒或者超时。当&lt;code&gt;GroupTransferService&lt;/code&gt;发现从broker已经同步的最后一个消息的索引大于本次消息的索引时就会唤醒&lt;code&gt;GroupCommitRequest&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;h4 id=&#34;写buffer&#34;&gt;写buffer&lt;/h4&gt;&#xA;&lt;p&gt;使用了写buffer以后，写消息的全部逻辑就是把消息写入buffer。同时，系统会有一个线程&lt;code&gt;CommitRealTimeService&lt;/code&gt;定期把消息写入文件。&lt;/p&gt;&#xA;&lt;h3 id=&#34;核心代码&#34;&gt;核心代码&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;org.apache.rocketmq.store.CommitLog&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;消费队列&#34;&gt;消费队列&lt;/h2&gt;&#xA;&lt;p&gt;每个topic对应多个消费队列，这个是提高消费并发度的前提。&lt;/p&gt;&#xA;&lt;h3 id=&#34;结构&#34;&gt;结构&lt;/h3&gt;&#xA;&lt;p&gt;每个消费队列对应一个逻辑文件，文件中对应每个消息的内容大小是固定的20个字节，包含消息的偏移量，大小以及tag哈希值。&lt;/p&gt;&#xA;&lt;h4 id=&#34;文件目录&#34;&gt;文件目录&lt;/h4&gt;&#xA;&lt;p&gt;数据保存在目录&lt;code&gt;${rootpath}/consumequeue&lt;/code&gt;下面，&lt;code&gt;rootpath&lt;/code&gt; 通过配置项&lt;code&gt;storePathRootDir&lt;/code&gt;指定，默认的是&lt;code&gt;${user.home}/store&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;${rootpath}/consumequeue&#xA;└── 0%default                     // topic&#xA;    ├── 0                         // queue 0&#xA;    │   └── 00000000000000000000&#xA;    ├── 1                         // queue 1&#xA;    │   └── 00000000000000000000&#xA;    ├── 2                         // queue 2&#xA;    │   └── 00000000000000000000&#xA;    └── 3                         // queue 3&#xA;        └── 00000000000000000000&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;队列元素&#34;&gt;队列元素&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;|&amp;lt;----- 8 byte -----&amp;gt;|&amp;lt;- 4 byte -&amp;gt;|&amp;lt;------ 8 byte ------&amp;gt;|&#xA;+--------------------+------------+----------------------+&#xA;|   commitlog offset |   size     | message tag hash code|&#xA;+--------------------+------------+----------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;执行&#34;&gt;执行&lt;/h3&gt;&#xA;&lt;p&gt;通过线程&lt;code&gt;ReputMessageService&lt;/code&gt;的分派消息的逻辑执行。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
